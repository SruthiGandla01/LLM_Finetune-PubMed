
# ðŸ§  Fine-Tuning FLAN-T5 on Biomedical Summarization (PubMed Dataset)

![Python](https://img.shields.io/badge/Python-3.12-blue?logo=python)
![Hugging Face](https://img.shields.io/badge/Transformers-ðŸ¤—-yellow)
![Google Colab](https://img.shields.io/badge/Google%20Colab-A100%20GPU-orange?logo=googlecolab)
![License](https://img.shields.io/badge/License-MIT-green)

---

## ðŸ“˜ Project Overview
This project demonstrates how to fine-tune a **Large Language Model (LLM)** â€” specifically **Googleâ€™s FLAN-T5-Small** â€” for **domain-specific text summarization** using the **PubMed Summarization** dataset.

The goal is to adapt a general instruction-tuned model to the **biomedical domain**, improving factual accuracy and relevance in generated summaries.

---

## ðŸ§© Key Features
- âœ… Fine-tuning FLAN-T5 on real-world biomedical abstracts (PubMed dataset)
- âœ… Data preprocessing, tokenization, and formatting for seq2seq tasks
- âœ… Hyperparameter optimization using **Ray Tune**
- âœ… Evaluation with **ROUGE metrics** and baseline comparison
- âœ… Error analysis and inference pipeline for live summarization
- âœ… Visualization of performance improvements
- âœ… Model saving, logging, and reproducibility (results, logs, wandb)

---

## âš™ï¸ Tech Stack
| Category | Tools / Libraries |
|-----------|------------------|
| Environment | Google Colab (A100 GPU) |
| Framework | ðŸ¤— Transformers, Datasets, Evaluate |
| Optimization | Ray Tune, EarlyStoppingCallback |
| Logging | TensorBoard, Weights & Biases (wandb) |
| Visualization | Matplotlib, Pandas |
| Language | Python 3.12 |

---


---

## ðŸ§® Dataset
**Source:** [ccdv/pubmed-summarization](https://huggingface.co/datasets/ccdv/pubmed-summarization)  
- Domain: Biomedical research papers (articles + abstracts)  
- Total records: ~133,000    
- Input length: 512 tokens  
- Target length: 128 tokens  

---

## ðŸ§  Model Details
**Base model:** `google/flan-t5-small`  
**Why FLAN-T5?**
- Instruction-tuned (better generalization)  
- Lightweight (77M params â†’ fits A100 easily)  
- Strong seq2seq summarization baseline  

### Training Configuration
| Parameter | Value |
|------------|--------|
| Learning Rate | 5e-5 |
| Batch Size | 4 |
| Epochs | 3 |
| Weight Decay | 0.01 |
| Evaluation Strategy | Epoch |
| Save Strategy | Epoch |
| Early Stopping | Patience = 2 |
| Metric | ROUGE-L |

---

## ðŸ” Evaluation Results

| Model | ROUGE-1 | ROUGE-2 | ROUGE-L | ROUGE-Lsum |
|--------|----------|----------|-----------|-------------|
| **Baseline (Pre-trained FLAN-T5)** | 0.0526 | 0.0085 | 0.0425 | 0.0475 |
| **Fine-Tuned (PubMed)** | **0.1131** | **0.0387** | **0.0886** | **0.0958** |

ðŸ“ˆ *The fine-tuned model achieved more than 2Ã— improvement across all metrics.*

---

## ðŸ“Š Visualization
Bar chart comparing ROUGE metrics:

ROUGE-1 â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“
ROUGE-2 â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“
ROUGE-L â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“
ROUGE-Lsum â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“


*(Fine-tuned scores outperform baseline in every category.)*

---

## âŒ Error Analysis

| Issue | Observation | Mitigation |
|--------|--------------|-------------|
| Truncation | Long articles >512 tokens lost details | Increase `max_input_length` to 768 |
| Factual drift | Biomedical entities occasionally paraphrased | Use domain tokenizer (e.g., SciBERT) |
| Generic phrasing | Summaries sometimes vague | Continue fine-tuning with LoRA/PEFT |

---
---
## âš¡ Setup & Reproducibility

### ðŸ§© 1. Install dependencies
```bash
pip install -r requirements.txt
```

### 2. Run the notebook
Open in Colab
```bash
https://colab.research.google.com/github/SruthiGandla101/LLM-Finetune-PubMed/blob/main/LLM_Finetune.ipynb
```
---
### 3. Resume training
If interrupted, checkpoints auto-resume:
```bash
trainer.train(resume_from_checkpoint=True)
```
---
---
##  ðŸ§© Lessons Learned

- Domain-specific fine-tuning significantly boosts LLM performance.
- Ray Tune enables fast hyperparameter exploration even in Colab.
- FLAN-T5-Small provides a strong, efficient baseline for biomedical text generation.

---
---
## ðŸš§ Future Work

- Fine-tune larger variants (FLAN-T5-Base / Large)
- Evaluate factual consistency using biomedical entity match scoring
- Deploy as a web demo (Streamlit or Hugging Face Spaces)

---
